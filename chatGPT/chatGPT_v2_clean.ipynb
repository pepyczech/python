{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Erqf5lp6Apj0",
        "outputId": "d8eda592-c41f-4d1c-c1f4-15667e201249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.4 yarl-1.9.1\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import openai\n",
        "except:\n",
        "  !pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "aFtI1NtrBrej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oai_key=\"<YOUR_OPENAI_API_KEY>\""
      ],
      "metadata": {
        "id": "wtq0b0QIBmq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(prompt,artefact=None,mask=None,model_engine = 'gpt-4',max_tokens=1024,secret_key=None,\n",
        "                      mode='create',test=0,persona=\"You are a helpful assistant.\",number=1,size='1024x1024'):\n",
        "\n",
        "    # https://github.com/openai/openai-python\n",
        "    # https://platform.openai.com/docs/models/overview\n",
        "    # https://platform.openai.com/docs/api-reference/introduction\n",
        "    # https://platform.openai.com/docs/api-reference/completions/create\n",
        "\n",
        "    # MODELS: 'gpt-4','gpt-3.5-turbo', \"text-davinci-003\",\"text-davinci-002\", \"code-davinci-002\", \"dall.e\"\n",
        "    # MODE: only for image generation/editing: https://platform.openai.com/docs/guides/images/introduction - 'create','edit','variation'\n",
        "\n",
        "    # If input is a text file\n",
        "    if prompt.split('.')[-1].lower() in ['txt','dat','sql','py','r']:\n",
        "        print('Reading ',prompt)\n",
        "        with open(prompt) as f:\n",
        "            prompt = f.read()\n",
        "\n",
        "    # If additional data provided\n",
        "    if artefact is not None:\n",
        "        print('Additional artefact provided...')\n",
        "        if artefact.split('.')[-1].lower() in ['txt','dat','sql','py','r']:\n",
        "            print('Reading ',artefact)\n",
        "            with open(artefact) as f:\n",
        "                artefact = f.read()\n",
        "            prompt=prompt+' '+artefact\n",
        "        elif artefact.split('.')[-1].lower() in ['csv']:\n",
        "            artefact = pd.read_csv(artefact)\n",
        "            artefact = ', '.join(list(artefact.iloc[:,0].astype(str)))\n",
        "            prompt=prompt+' '+artefact\n",
        "        elif artefact.split('.')[-1].lower() in ['png','jpg','jpeg']:\n",
        "            artefact=open(artefact, \"rb\")\n",
        "        elif isinstance(artefact,str):\n",
        "            prompt=prompt+' '+artefact\n",
        "\n",
        "    if mask is not None:\n",
        "      if mask.split('.')[-1].lower() in ['png','jpg','jpeg']:\n",
        "        mask=open(mask, \"rb\")\n",
        "\n",
        "    result = 'INVALID OPENAI KEY!!!'\n",
        "    completions = None\n",
        "\n",
        "    if secret_key is not None:\n",
        "\n",
        "        print('>>> Querying chatGPT ',model_engine)\n",
        "        #print(prompt)\n",
        "\n",
        "        #prompt = (f\"{prompt}\")\n",
        "\n",
        "        client = OpenAI(\n",
        "          # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "          api_key=secret_key,\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            if np.sum(test)==0:\n",
        "                if ('dall-e' in model_engine.lower())&(mode.lower()=='create'):\n",
        "\n",
        "                    completions = client.images.generate(\n",
        "                                  model=model_engine.lower(),\n",
        "                                  prompt=prompt,\n",
        "                                  n=number,\n",
        "                                  size=size\n",
        "                                )\n",
        "                    result = completions['data'][0]['url']\n",
        "                elif ('dall-e' in model_engine.lower())&(mode.lower()=='edit'):\n",
        "                    completions = client.images.edit(\n",
        "                                  image=artefact,\n",
        "                                  mask=mask,\n",
        "                                  prompt=prompt,\n",
        "                                  n=number,\n",
        "                                  size=size\n",
        "                                )\n",
        "                    result = completions['data'][0]['url']\n",
        "                elif ('dall-e' in model_engine.lower())&(mode == 'variation'):\n",
        "                    completions = client.images.create_variation(\n",
        "                                image=artefact,\n",
        "                                n=number,\n",
        "                                size=size\n",
        "                              )\n",
        "                    result = completions['data'][0]['url']\n",
        "                else:\n",
        "                    completions = client.chat.completions.create(\n",
        "                              model=model_engine.lower(),\n",
        "                              messages=[{\"role\": \"system\", \"content\": persona},\n",
        "                                        {\"role\": \"user\", \"content\": prompt}],\n",
        "                              max_tokens=max_tokens,\n",
        "                              n=number,\n",
        "                              stop=None,\n",
        "                              temperature=0.5\n",
        "                          )\n",
        "                    result = completions.choices[0].message.content\n",
        "                    result = result.strip()\n",
        "\n",
        "                print('>>> Query completed successfully :)')\n",
        "                print('Token usage: ')\n",
        "                print(completions.usage)\n",
        "            else:\n",
        "                print(\"API not queried - only testing...\")\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            exc_type, exc_value, exc_traceback = sys.exc_info()\n",
        "            traceback.print_exception(exc_type, exc_value, exc_traceback,limit=5, file=sys.stdout)\n",
        "            result=e\n",
        "    else:\n",
        "        print('INVALID OPENAI KEY!!!')\n",
        "\n",
        "    return(result,completions)"
      ],
      "metadata": {
        "id": "2QEBBl-jBFrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='How many moons does Jupyter have?'\n",
        "answer,compl=generate_response(prompt,model_engine = \"text-davinci-003\",max_tokens=1024,secret_key=oai_key,test=0)\n",
        "print(answer)\n",
        "print(compl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI2YTmQoCG3s",
        "outputId": "020f5b3c-321f-4d0b-9fac-e707f6543161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How many moons does Jupyter have?\n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \"\\n\\nJupyter does not have any moons.\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1679094232,\n",
            "  \"id\": \"cmpl-6vDOyOAuENyIG44YuJma05RovXHxq\",\n",
            "  \"model\": \"text-davinci-003\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 12,\n",
            "    \"prompt_tokens\": 10,\n",
            "    \"total_tokens\": 22\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"Please explain what the following function does:\"\n",
        "\n",
        "artefact='''\n",
        "def solution(s):\n",
        "    for i in range(len(s)):\n",
        "        t = s[:i] + s[i+1:]\n",
        "        if t == t[::-1]: return True\n",
        "\n",
        "    return s == s[::-1]'''\n",
        "\n",
        "answer,compl=generate_response(prompt,artefact=artefact,model_engine = \"gpt-3.5-turbo\",max_tokens=200,secret_key=oai_key,test=0)\n",
        "print(answer)\n",
        "print('**********************')\n",
        "print(compl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTeHJqGt5ZZX",
        "outputId": "f1d513d9-accd-4c3c-8aa6-6633ff67e63d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Additional artefact provided...\n",
            ">>> Querying chatGPT  gpt-3.5-turbo  model with the following prompt:\n",
            "Please explain what the following function do: \n",
            "def solution(s):\n",
            "    for i in range(len(s)):\n",
            "        t = s[:i] + s[i+1:]\n",
            "        if t == t[::-1]: return True\n",
            "\n",
            "    return s == s[::-1]\n",
            ">>> Query completed successfully :)\n",
            "Token usage: \n",
            "{\n",
            "  \"completion_tokens\": 200,\n",
            "  \"prompt_tokens\": 71,\n",
            "  \"total_tokens\": 271\n",
            "}\n",
            "This function is a solution to check if a given string `s` is a palindrome or not. \n",
            "\n",
            "It works as follows:\n",
            "- It loops through each character in the string `s` using the `range` function and the `len` function to get the length of the string.\n",
            "- For each character, it creates a new string `t` by slicing the original string `s` from the beginning up to the current character (excluding it) and concatenating it with the rest of the string (excluding the current character).\n",
            "- It then checks if the new string `t` is equal to its reverse using the `[::-1]` slicing operation. \n",
            "- If the new string `t` is a palindrome, it returns `True`.\n",
            "- If no palindromes are found, it checks if the original string `s` is a palindrome by comparing it to its reverse using the `[::-1]` slicing operation. If it is a palindrome, it returns `True\n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"length\",\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"content\": \"This function is a solution to check if a given string `s` is a palindrome or not. \\n\\nIt works as follows:\\n- It loops through each character in the string `s` using the `range` function and the `len` function to get the length of the string.\\n- For each character, it creates a new string `t` by slicing the original string `s` from the beginning up to the current character (excluding it) and concatenating it with the rest of the string (excluding the current character).\\n- It then checks if the new string `t` is equal to its reverse using the `[::-1]` slicing operation. \\n- If the new string `t` is a palindrome, it returns `True`.\\n- If no palindromes are found, it checks if the original string `s` is a palindrome by comparing it to its reverse using the `[::-1]` slicing operation. If it is a palindrome, it returns `True\",\n",
            "        \"role\": \"assistant\"\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1679098562,\n",
            "  \"id\": \"chatcmpl-6vEWoxh3OinLaWPa4hgHZvjRJjiK2\",\n",
            "  \"model\": \"gpt-3.5-turbo-0301\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 200,\n",
            "    \"prompt_tokens\": 71,\n",
            "    \"total_tokens\": 271\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='''Given an array of numbers,\n",
        "          write a python function to move all zeroes in the array to the end of it\n",
        "          while maintaining the relative order of the non-zero elements.'''\n",
        "\n",
        "answer,compl=generate_response(prompt,model_engine = \"gpt-3.5-turbo\",max_tokens=500,secret_key=oai_key,test=0)\n",
        "print(answer)\n",
        "print(compl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxlLO_3F8TGy",
        "outputId": "8bc1bb7b-ffd2-4bc5-d1a5-553d5b402b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Querying chatGPT  gpt-3.5-turbo  model with the following prompt:\n",
            "Given an array of numbers, write a python function to move all zeroes in the array to the end of it while maintaining the relative order of the non-zero elements.\n",
            ">>> Query completed successfully :)\n",
            "Token usage: \n",
            "{\n",
            "  \"completion_tokens\": 190,\n",
            "  \"prompt_tokens\": 51,\n",
            "  \"total_tokens\": 241\n",
            "}\n",
            "Here is a Python function that moves all zeroes in the array to the end while maintaining the order of the non-zero elements:\n",
            "\n",
            "```python\n",
            "def move_zeroes(arr):\n",
            "    # initialize a counter for non-zero elements\n",
            "    count = 0\n",
            "    \n",
            "    # iterate through the array and move non-zero elements to the front\n",
            "    for i in range(len(arr)):\n",
            "        if arr[i] != 0:\n",
            "            arr[count] = arr[i]\n",
            "            count += 1\n",
            "    \n",
            "    # fill the remaining elements with zeroes\n",
            "    while count < len(arr):\n",
            "        arr[count] = 0\n",
            "        count += 1\n",
            "    \n",
            "    return arr\n",
            "```\n",
            "\n",
            "Here's an example of how to use the function:\n",
            "\n",
            "```python\n",
            "arr = [0, 1, 0, 3, 12]\n",
            "print(move_zeroes(arr))  # Output: [1, 3, 12, 0, 0]\n",
            "```\n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"content\": \"\\n\\nHere is a Python function that moves all zeroes in the array to the end while maintaining the order of the non-zero elements:\\n\\n```python\\ndef move_zeroes(arr):\\n    # initialize a counter for non-zero elements\\n    count = 0\\n    \\n    # iterate through the array and move non-zero elements to the front\\n    for i in range(len(arr)):\\n        if arr[i] != 0:\\n            arr[count] = arr[i]\\n            count += 1\\n    \\n    # fill the remaining elements with zeroes\\n    while count < len(arr):\\n        arr[count] = 0\\n        count += 1\\n    \\n    return arr\\n```\\n\\nHere's an example of how to use the function:\\n\\n```python\\narr = [0, 1, 0, 3, 12]\\nprint(move_zeroes(arr))  # Output: [1, 3, 12, 0, 0]\\n```\",\n",
            "        \"role\": \"assistant\"\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1679098465,\n",
            "  \"id\": \"chatcmpl-6vEVFmOYkC0eMN59CBc4Rg5MuctRQ\",\n",
            "  \"model\": \"gpt-3.5-turbo-0301\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 190,\n",
            "    \"prompt_tokens\": 51,\n",
            "    \"total_tokens\": 241\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function\n",
        "def move_zeroes(arr):\n",
        "    # initialize a counter for non-zero elements\n",
        "    count = 0\n",
        "\n",
        "    # iterate through the array and move non-zero elements to the front\n",
        "    for i in range(len(arr)):\n",
        "        if arr[i] != 0:\n",
        "            arr[count] = arr[i]\n",
        "            count += 1\n",
        "\n",
        "    # fill the remaining elements with zeroes\n",
        "    while count < len(arr):\n",
        "        arr[count] = 0\n",
        "        count += 1\n",
        "\n",
        "    return arr\n",
        "\n",
        "move_zeroes([1,0,2,0,3,0,4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMTU2VALQwYa",
        "outputId": "458ce66f-bf26-49b6-f020-b9fbd5a3fdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"Write an Oracle SQL query that pulls all columns from table TABLE, filters only rows with value DATE greater than 28th Jun 2013 and then returns only rows 200 to 300 of the filtered data set\"\n",
        "\n",
        "answer,compl=generate_response(prompt,model_engine = \"gpt-3.5-turbo\",max_tokens=500,secret_key=oai_key,test=0)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6j5YeoSQ--n",
        "outputId": "eb90a04a-ed1d-4dc2-ec0a-6ca3222123a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Querying chatGPT  gpt-3.5-turbo  model with the following prompt:\n",
            "Write an Oracle SQL query that pulls all columns from table TABLE, filters only rows with value DATE greater than 28th Jun 2013 and then returns only rows 200 to 300 of the filtered data set\n",
            ">>> Query completed successfully :)\n",
            "Token usage: \n",
            "{\n",
            "  \"completion_tokens\": 118,\n",
            "  \"prompt_tokens\": 61,\n",
            "  \"total_tokens\": 179\n",
            "}\n",
            "Here's the SQL query you can use:\n",
            "\n",
            "```\n",
            "SELECT *\n",
            "FROM (\n",
            "  SELECT ROW_NUMBER() OVER (ORDER BY DATE) AS row_num, TABLE.*\n",
            "  FROM TABLE\n",
            "  WHERE DATE > '2013-06-28'\n",
            ")\n",
            "WHERE row_num BETWEEN 200 AND 300;\n",
            "```\n",
            "\n",
            "This query uses a subquery to filter the data based on the DATE column, and then uses the ROW_NUMBER() function to assign a row number to each row in the filtered data set. Finally, the outer query selects only rows with row numbers between 200 and 300.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='''Write an Oracle SQL query that applies regexp_like function\n",
        "          to columns COL1, COL2, COL5 and COL8 of table TABLE\n",
        "          to search for pattern 'VFC1|VFP3|VFC4'\n",
        "          and returns all rows of the table that return at least one match'''\n",
        "\n",
        "answer,compl=generate_response(prompt,model_engine = \"gpt-3.5-turbo\",max_tokens=500,secret_key=oai_key,test=0)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhJFBGs9UJys",
        "outputId": "00a886fe-c07e-4417-f457-9bb0bbcaa6c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Querying chatGPT  gpt-3.5-turbo  model with the following prompt:\n",
            "Write an Oracle SQL query that applies regexp_like function to columns COL1, COL2, COL5 and COL8 of table TABLE to search for pattern 'VFC1|VFP3|VFC4' and returns all rows of the table that return at least one match\n",
            ">>> Query completed successfully :)\n",
            "Token usage: \n",
            "{\n",
            "  \"completion_tokens\": 154,\n",
            "  \"prompt_tokens\": 74,\n",
            "  \"total_tokens\": 228\n",
            "}\n",
            "Here is the SQL query that applies regexp_like function to columns COL1, COL2, COL5 and COL8 of table TABLE to search for pattern 'VFC1|VFP3|VFC4' and returns all rows of the table that return at least one match:\n",
            "\n",
            "```\n",
            "SELECT *\n",
            "FROM TABLE\n",
            "WHERE REGEXP_LIKE(COL1 || COL2 || COL5 || COL8, 'VFC1|VFP3|VFC4');\n",
            "```\n",
            "\n",
            "This query concatenates the values of columns COL1, COL2, COL5 and COL8 into one string and then applies the REGEXP_LIKE function to search for the specified pattern. The WHERE clause filters the results to only include rows where at least one match was found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='''I want you to act as a programmer in Python. Let df be a very large dataframe with more than 1 million rows.\n",
        "          Please simplify this code while ensuring that it is more efficient and explain why.'''\n",
        "\n",
        "artefact='''\n",
        "        for i in len(df):\n",
        "            if df['column1']>8:\n",
        "                df.loc[i,'column2']=8\n",
        "            else:\n",
        "                df.loc[i,'column2']=-20\n",
        "\n",
        "        '''\n",
        "\n",
        "answer,compl=generate_response(prompt,artefact=artefact,model_engine = \"gpt-3.5-turbo\",max_tokens=200,secret_key=oai_key,test=0)\n",
        "print(answer)\n",
        "print('**********************')\n",
        "print(compl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oGhjbT6OXEH",
        "outputId": "29bfcd23-1574-4f6c-add5-ff7108fb038b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Additional artefact provided...\n",
            ">>> Querying chatGPT  gpt-3.5-turbo  model with the following prompt:\n",
            "I want you to act as a programmer in Python. Let df be a very large dataframe with more than 1 million rows.\n",
            "          Please simplify this code while ensuring that it is more efficient and explain why. \n",
            "        for i in len(df):\n",
            "            if df['column1']>8:\n",
            "                df.loc[i,'column2']=8\n",
            "            else:\n",
            "                df.loc[i,'column2']=-20\n",
            "\n",
            "        \n",
            ">>> Query completed successfully :)\n",
            "Token usage: \n",
            "{\n",
            "  \"completion_tokens\": 167,\n",
            "  \"prompt_tokens\": 103,\n",
            "  \"total_tokens\": 270\n",
            "}\n",
            "Here's a simplified and more efficient version of the code you provided:\n",
            "\n",
            "```\n",
            "df.loc[df['column1'] > 8, 'column2'] = 8\n",
            "df.loc[df['column1'] <= 8, 'column2'] = -20\n",
            "```\n",
            "\n",
            "This code uses boolean indexing to locate the rows where `column1` is greater than 8 and sets the corresponding values in `column2` to 8. Similarly, it sets the values in `column2` to -20 for the rows where `column1` is less than or equal to 8.\n",
            "\n",
            "This code is more efficient because it avoids the use of a loop by using vectorized operations that are optimized for large dataframes. It also reduces the number of calls to `df.loc`, which can be expensive for large dataframes.\n",
            "**********************\n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"content\": \"Here's a simplified and more efficient version of the code you provided:\\n\\n```\\ndf.loc[df['column1'] > 8, 'column2'] = 8\\ndf.loc[df['column1'] <= 8, 'column2'] = -20\\n```\\n\\nThis code uses boolean indexing to locate the rows where `column1` is greater than 8 and sets the corresponding values in `column2` to 8. Similarly, it sets the values in `column2` to -20 for the rows where `column1` is less than or equal to 8.\\n\\nThis code is more efficient because it avoids the use of a loop by using vectorized operations that are optimized for large dataframes. It also reduces the number of calls to `df.loc`, which can be expensive for large dataframes.\",\n",
            "        \"role\": \"assistant\"\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1681803246,\n",
            "  \"id\": \"chatcmpl-76a8kH3WQZHQrgWayntxTAAmgKdXV\",\n",
            "  \"model\": \"gpt-3.5-turbo-0301\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 167,\n",
            "    \"prompt_tokens\": 103,\n",
            "    \"total_tokens\": 270\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='''I want you to act as a programmer in R.  Please translate this code to Python and explain what it does'''\n",
        "\n",
        "artefact='''\n",
        "pow <- function(x, y) {\n",
        "result <- x^y\n",
        "print(paste(\"Result: \", result))\n",
        "}'''\n",
        "\n",
        "answer,compl=generate_response(prompt,artefact=artefact,model_engine = \"gpt-3.5-turbo\",max_tokens=200,secret_key=oai_key,test=0)\n",
        "print(answer)\n",
        "print('**********************')\n",
        "print(compl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKeUrDJZOd-N",
        "outputId": "03e94340-cb36-4ccb-c8ca-60b416d0a1c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Additional artefact provided...\n",
            ">>> Querying chatGPT  gpt-3.5-turbo  model with the following prompt:\n",
            "I want you to act as a programmer in R.  Please translate this code to Python and explain what it does \n",
            "pow <- function(x, y) {\n",
            "result <- x^y\n",
            "print(paste(\"Result: \", result))\n",
            "}\n",
            ">>> Query completed successfully :)\n",
            "Token usage: \n",
            "{\n",
            "  \"completion_tokens\": 138,\n",
            "  \"prompt_tokens\": 67,\n",
            "  \"total_tokens\": 205\n",
            "}\n",
            "Certainly! Here is the equivalent code in Python:\n",
            "\n",
            "```python\n",
            "def pow(x, y):\n",
            "    result = x ** y\n",
            "    print(\"Result: \", result)\n",
            "```\n",
            "\n",
            "This code defines a function called `pow` that takes two arguments, `x` and `y`. Within the function, the result of raising `x` to the power of `y` is calculated using the `**` operator and stored in a variable called `result`. The function then prints a message to the console using the `print` function, which includes the result of the calculation.\n",
            "\n",
            "Overall, this function provides a simple way to calculate and display the result of raising a number to a power.\n",
            "**********************\n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"content\": \"Certainly! Here is the equivalent code in Python:\\n\\n```python\\ndef pow(x, y):\\n    result = x ** y\\n    print(\\\"Result: \\\", result)\\n```\\n\\nThis code defines a function called `pow` that takes two arguments, `x` and `y`. Within the function, the result of raising `x` to the power of `y` is calculated using the `**` operator and stored in a variable called `result`. The function then prints a message to the console using the `print` function, which includes the result of the calculation.\\n\\nOverall, this function provides a simple way to calculate and display the result of raising a number to a power.\",\n",
            "        \"role\": \"assistant\"\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1681803362,\n",
            "  \"id\": \"chatcmpl-76aAc9r6JrZhysKKpTimm2TIzhojY\",\n",
            "  \"model\": \"gpt-3.5-turbo-0301\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 138,\n",
            "    \"prompt_tokens\": 67,\n",
            "    \"total_tokens\": 205\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pow(x, y):\n",
        "    result = x ** y\n",
        "    print(\"Result: \", result)\n",
        "\n",
        "pow(2,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UphAXRrKSysn",
        "outputId": "b3f0696e-3d96-444b-d158-eeb1cf0c5af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result:  8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='''I want you to act as a programmer in Python.\n",
        "          Please correct the errors in the code and explain how these errors would affect the code'''\n",
        "\n",
        "artefact='''\n",
        "        for i in df.shape:\n",
        "            if df['column1']=8:\n",
        "                    df.loc[i,'column2']=8\n",
        "            else df['column1']>8:\n",
        "                df.loc[j,'column2']==-20\n",
        "            elseif:\n",
        "                df.iloc[i,'column2']=1e4\n",
        "\n",
        "        '''\n",
        "\n",
        "answer,compl=generate_response(prompt,artefact=artefact,model_engine = \"gpt-3.5-turbo\",max_tokens=200,secret_key=oai_key,test=0)\n",
        "print(answer)\n",
        "print('**********************')\n",
        "print(compl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq7lcuFVRbpl",
        "outputId": "f7a1be9e-964f-4fd2-9837-e75b719689d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Additional artefact provided...\n",
            ">>> Querying chatGPT  gpt-3.5-turbo  model with the following prompt:\n",
            "I want you to act as a programmer in Python. \n",
            "          Please correct the errors in the code and explain how these errors would affect the code \n",
            "        for i in df.shape:\n",
            "            if df['column1']=8:\n",
            "                    df.loc[i,'column2']=8\n",
            "            else df['column1']>8:\n",
            "                df.loc[j,'column2']==-20\n",
            "            elseif:\n",
            "                df.iloc[i,'column2']=1e4\n",
            "\n",
            "        \n",
            ">>> Query completed successfully :)\n",
            "Token usage: \n",
            "{\n",
            "  \"completion_tokens\": 200,\n",
            "  \"prompt_tokens\": 111,\n",
            "  \"total_tokens\": 311\n",
            "}\n",
            "Here is the corrected code with explanations of the errors:\n",
            "\n",
            "```python\n",
            "for i in range(df.shape[0]): # use range() to iterate over the rows of the DataFrame\n",
            "    if df.loc[i, 'column1'] == 8: # use == for comparison and specify the row and column of the DataFrame\n",
            "        df.loc[i, 'column2'] = 8\n",
            "    elif df.loc[i, 'column1'] > 8: # use elif instead of else if and specify the row and column of the DataFrame\n",
            "        df.loc[i, 'column2'] = -20\n",
            "    else: # use else instead of elseif\n",
            "        df.loc[i, 'column2'] = 1e4 # use loc() to specify the row and column of the DataFrame\n",
            "```\n",
            "\n",
            "Explanation of the errors:\n",
            "\n",
            "1. The `for` loop is iterating over the shape of the DataFrame, which returns a tuple of the number of rows and columns. To iterate over the rows\n",
            "**********************\n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"length\",\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"content\": \"Here is the corrected code with explanations of the errors:\\n\\n```python\\nfor i in range(df.shape[0]): # use range() to iterate over the rows of the DataFrame\\n    if df.loc[i, 'column1'] == 8: # use == for comparison and specify the row and column of the DataFrame\\n        df.loc[i, 'column2'] = 8\\n    elif df.loc[i, 'column1'] > 8: # use elif instead of else if and specify the row and column of the DataFrame\\n        df.loc[i, 'column2'] = -20\\n    else: # use else instead of elseif\\n        df.loc[i, 'column2'] = 1e4 # use loc() to specify the row and column of the DataFrame\\n```\\n\\nExplanation of the errors:\\n\\n1. The `for` loop is iterating over the shape of the DataFrame, which returns a tuple of the number of rows and columns. To iterate over the rows\",\n",
            "        \"role\": \"assistant\"\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1681803572,\n",
            "  \"id\": \"chatcmpl-76aE03fD8HeNj3oQRbgjIUF6PzG5e\",\n",
            "  \"model\": \"gpt-3.5-turbo-0301\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 200,\n",
            "    \"prompt_tokens\": 111,\n",
            "    \"total_tokens\": 311\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}